{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVYFQ9QNS3hi"
   },
   "source": [
    "# **Malicious URLs Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPBtvmMSTOwV"
   },
   "source": [
    "## Step 1: Handle imports and import CSV file from shareable link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YR7f8wianTTT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "\n",
    "from sklearn.dummy import DummyClassifier # baseline model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler # data is normal distributed, hence using StandardScaler\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from typing import Tuple, Union\n",
    "\n",
    "from scipy.stats import kstest\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from tld import get_tld, is_tld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "KNnI6-2zUHUa",
    "outputId": "befd6fa1-715e-4619-8ccb-d22dea1551e7"
   },
   "outputs": [],
   "source": [
    "# Read csv file to dataframe\n",
    "df = pd.read_csv(\"malicious_phish.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNxKr8KMX3v6"
   },
   "source": [
    "## Step 2: Gain understanding of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kblcidc-awud",
    "outputId": "8c7979ed-b495-4f27-f9f1-1ada56471380"
   },
   "outputs": [],
   "source": [
    "# Initial information about the dataset, i.e. columns number, column name, count, and dtype\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GnD0suUHdUEz",
    "outputId": "fc9fa969-cff1-468b-cd44-516c6288eec5"
   },
   "outputs": [],
   "source": [
    "# How many null values are present\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bYqUKh-IZ3U6",
    "outputId": "373c0809-5576-488f-87ba-20d5c5bbdc10"
   },
   "outputs": [],
   "source": [
    "# Store and display value counts\n",
    "value_counts = df.type.value_counts()\n",
    "value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hldAuGX3VG1i",
    "outputId": "da31d73a-d369-4df9-e5b0-6754e6842808"
   },
   "outputs": [],
   "source": [
    "# Map every category found in df[\"type\"] to numerical categorical variable\n",
    "numerical_categories = {}\n",
    "for index, category in enumerate(value_counts.index):\n",
    "    numerical_categories[f\"{category}\"] = index\n",
    "\n",
    "# Create new feature with target variable corresponding to numerical_categories\n",
    "df[\"target\"] = [numerical_categories[category] for category in df[\"type\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "g_qxqnKTdp2k",
    "outputId": "861b6ba4-47dc-4182-8337-95501a24e11a"
   },
   "outputs": [],
   "source": [
    "# Nice little color palette, we can use throughout the paper\n",
    "colors = sns.color_palette(\"pastel\")[0:5]\n",
    "\n",
    "# Donut chart for the target variable ratios\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "plt.pie(value_counts, labels=value_counts.index, colors=colors, autopct=\"%.0f%%\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "sy1aocDgOCDp",
    "outputId": "eb7fed80-a6ef-4a18-f07c-fece3fc53f26"
   },
   "outputs": [],
   "source": [
    "# Supplied barplot visualisation\n",
    "sns.barplot(x=value_counts.index, y=value_counts, palette=colors)\n",
    "plt.xlabel(\"target\")\n",
    "plt.ylabel(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7tMopblP7Jd"
   },
   "source": [
    "## Step 3: Data preprocessing and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gsP_z61HSnn3"
   },
   "outputs": [],
   "source": [
    "# Function to map new feature based on whether ip address in url\n",
    "# courtesy of https://www.kaggle.com/code/jingyanshang/url-s-feature-analysis/notebook\n",
    "def url_is_ip_address(url: str) -> int:\n",
    "    match = re.search(\n",
    "            '(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.'\n",
    "            '([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\/)|'  # IPv4\n",
    "            '(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.'\n",
    "            '([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\/)|'  # IPv4 with port\n",
    "            '((0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\/)' # IPv4 in hexadecimal\n",
    "            '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}|'\n",
    "            '([0-9]+(?:\\.[0-9]+){3}:[0-9]+)|'\n",
    "            '((?:(?:\\d|[01]?\\d\\d|2[0-4]\\d|25[0-5])\\.){3}(?:25[0-5]|2[0-4]\\d|[01]?\\d\\d|\\d)(?:\\/\\d{1,2})?)', url)  # Ipv6\n",
    "\n",
    "    if match:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Function to map new feature based on http[s] protocol\n",
    "def https_secured(url: str) -> int:\n",
    "    try:\n",
    "        protocol = re.search(\"^(http|https)://\", url)\n",
    "\n",
    "        if protocol == None:\n",
    "            return 0\n",
    "        elif protocol.group(1) == \"https\":\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except Exception as err:\n",
    "        print(f\"[Error]: {err}\")\n",
    "        return 0\n",
    "\n",
    "def count_digits(url: str) -> int:\n",
    "    digits = 0\n",
    "    for x in url:\n",
    "        if x.isnumeric():\n",
    "            digits += 1\n",
    "    return digits\n",
    "\n",
    "def count_letters(url: str) -> int:\n",
    "    letters = 0\n",
    "    for x in url:\n",
    "        if x.isalpha():\n",
    "            letters += 1\n",
    "    \n",
    "    return letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hFFV9LwLS0l"
   },
   "outputs": [],
   "source": [
    "def process_tld(url: str, fix_protos: bool = False) -> Tuple[str, str, str, str]:\n",
    "    res = get_tld(url, as_object=True, fail_silently=False, fix_protocol=fix_protos)\n",
    "\n",
    "    subdomain = res.subdomain\n",
    "    domain = res.domain\n",
    "    tld = res.tld\n",
    "    fld = res.fld\n",
    "\n",
    "    return subdomain, domain, tld, fld\n",
    "\n",
    "def process_url_with_tld(row: pd.Series) -> Tuple[str, str, str, str]:\n",
    "    try:\n",
    "        if row[\"is_ip\"] == 0:\n",
    "            return process_tld(row[\"url\"], fix_protos=True)\n",
    "\n",
    "        else:\n",
    "            subdomain = domain = tld = fld = None\n",
    "\n",
    "            return subdomain, domain, tld, fld\n",
    "    except Exception as err:\n",
    "        return None, None, None, None\n",
    "\n",
    "def contains_shortening_service(url: str) -> int:\n",
    "    match = re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
    "                      'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
    "                      'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                      'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                      'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                      'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                      'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|'\n",
    "                      'tr\\.im|link\\.zip\\.net',\n",
    "                      url)\n",
    "    \n",
    "    if match:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Function for returning length, even if text is None\n",
    "def count_len(text: str) -> int:\n",
    "    if text == None:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "KTC_Lxzwlmn0",
    "outputId": "933b76b5-876c-4d06-d816-bcffc20990e6"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# For every url, check if ip address is embedded within\n",
    "df[\"is_ip\"] = df[\"url\"].apply(lambda x: url_is_ip_address(x))\n",
    "\n",
    "# For every url, check if http[s] is present and map to new feature\n",
    "df[\"is_https_secured\"] = df[\"url\"].apply(lambda x: https_secured(x))\n",
    "\n",
    "# Count numbers of characters inherent in url string\n",
    "df[\"url_length\"] = df[\"url\"].apply(lambda x: len(x))\n",
    "\n",
    "# Extract the primary domain from the url\n",
    "df[['subdomain', 'domain', 'tld', 'fld']] = df.apply(lambda x: process_url_with_tld(x), axis=1, result_type=\"expand\")\n",
    "\n",
    "\"\"\"\n",
    "All rows missing ['subdomain', 'domain', 'tld', 'fld'] (that is, because they\n",
    "are invalid) will return 0, because they don't adhere to the standard\n",
    "- most of them are likely to return 0 if the url is an ip address\n",
    "\"\"\"\n",
    "df[\"subdomain_len\"] = df[\"subdomain\"].apply(lambda x: count_len(x))\n",
    "df[\"tld_len\"] = df[\"tld\"].apply(lambda x: count_len(x))\n",
    "df[\"fld_len\"] = df[\"fld\"].apply(lambda x: count_len(x))\n",
    "\n",
    "# Count the digits in the url\n",
    "df[\"digit_count\"] = df[\"url\"].apply(lambda x: count_digits(x))\n",
    "\n",
    "# Count the letters in the url\n",
    "df[\"letter_count\"] = df[\"url\"].apply(lambda x: count_letters(x))\n",
    "\n",
    "# Count selected special characters, excluding directory seperator single slash\n",
    "special_characters = ['@','?','-','=','.','#','%','+','$','!','*',',','//']\n",
    "\n",
    "for char in special_characters:\n",
    "    df[char] = df[\"url\"].apply(lambda x: x.count(char))\n",
    "\n",
    "df[\"contains_shortening\"] = df[\"url\"].apply(lambda x: contains_shortening_service(x))\n",
    "\n",
    "# Total count of special characters in the url\n",
    "df[\"special_count\"] = df[special_characters].sum(axis=1)\n",
    "\n",
    "print(f\"time elapsed: {(time.time() - start):.2f} seconds\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "fAefg9NaA0N5",
    "outputId": "f3c1024d-7488-49fb-f582-e2f4068d22b5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df.corr(), linewidths=.5, cmap=sns.color_palette(\"pastel\"), annot=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvbqDdsIuzPe"
   },
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "df.to_csv(\"final_malicious_phish.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7xr-mpXxFsW"
   },
   "source": [
    "## Step 4: Create X and y subsets; Create training and testing subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "raMxoxDo2Aue"
   },
   "outputs": [],
   "source": [
    "# method for reducing memory size of dataframe (https://www.kaggle.com/code/arjanso/reducing-dataframe-memory-size-by-65/notebook)\n",
    "def reduce_memory(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    current_memory_usage = df.memory_usage().sum() / 1024 ** 2 # convert bytes to MB\n",
    "    print(f\"Memory of dataframe: {current_memory_usage} MB\\n\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != object:\n",
    "\n",
    "            print(\"**********************\")\n",
    "            print(f\"Column: {col}\")\n",
    "            print(\"----------------------\")\n",
    "            print(f\"Dtype (before): {df[col].dtype}\")\n",
    "            print(f\"Memory (before): {df[col].memory_usage() / 1024 ** 2:.2f} MB\")\n",
    "            print(\"----------------------\")\n",
    "\n",
    "            is_int = False\n",
    "\n",
    "            min, max = df[col].min(), df[col].max()\n",
    "            \n",
    "            to_int = df[col].fillna(0).astype(np.int64)\n",
    "            res = (df[col] - to_int)\n",
    "            res = res.sum()\n",
    "            if res > -0.01 and res < 0.01:\n",
    "                is_int = True\n",
    "\n",
    "            if is_int:\n",
    "                if min >= 0:\n",
    "                    if max < 255:\n",
    "                        df[col] = df[col].astype(np.uint8)\n",
    "                    elif max < 65535:\n",
    "                        df[col] = df[col].astype(np.uint16)\n",
    "                    elif max < 4294967295:\n",
    "                        df[col] = df[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if min > np.iinfo(np.int8).min and max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif min > np.iinfo(np.int16).min and max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif min > np.iinfo(np.int32).min and max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif min > np.iinfo(np.int64).min and max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "\n",
    "            print(f\"Dtype (after): {df[col].dtype}\")\n",
    "            print(f\"memory (after): {df[col].memory_usage() / 1024 ** 2:.2f} MB\")\n",
    "            print(\"**********************\\n\")\n",
    "\n",
    "    new_memory_usage = df.memory_usage().sum() / 1024 ** 2\n",
    "    print(f\"Memory usage of {current_memory_usage:.2f} MB reduced to {new_memory_usage:.2f} MB\")\n",
    "    print(f\"% reduced: {100 - (100 * new_memory_usage/current_memory_usage):.2f}%\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "O2TUaKn4V8P2",
    "outputId": "b90a6989-6416-439e-d976-f0a585c8e099"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_malicious_phish.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFeJDFmkSpM6"
   },
   "outputs": [],
   "source": [
    "# method to reduce dataframe by a fraction, while preserving target variable distribution\n",
    "def shorten_dataframe(df: pd.DataFrame, column: str, fraction: float) -> pd.DataFrame:\n",
    "    final_df = df.iloc[0:0]\n",
    "    \n",
    "    print(f\"Target column: {column}\\nShorten to {fraction * 100}%\\n\")\n",
    "\n",
    "    for index in df[column].value_counts().index:\n",
    "        final_df = pd.concat([final_df, df[df[column] == index].sample(frac=fraction, random_state=42)])\n",
    "        print(f\"{index} reduced from {len(df[df[column] == index])} to {len(final_df[final_df[column] == index])}\")\n",
    "\n",
    "    print(f\"\\n{final_df[column].value_counts()}\")\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tasT4cV5a26o",
    "outputId": "969d7cca-a79c-4940-fd97-ee5ad4939e33"
   },
   "outputs": [],
   "source": [
    "final_df = shorten_dataframe(df, column=\"target\", fraction=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7TYrm4K7tESS",
    "outputId": "e32676f8-7a29-4305-a7d2-1c7648ef86c8"
   },
   "outputs": [],
   "source": [
    "# change from multi-class to binary class, because either malicious or not\n",
    "final_df[\"target\"] = df[\"target\"].replace([2, 3], 1)\n",
    "final_df.url_length.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oD6TAZsz1YUj",
    "outputId": "4f0d28ab-f0d7-4752-c429-5a32d48e5c0a"
   },
   "outputs": [],
   "source": [
    "reduce_memory(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tj9iS7c5nzup"
   },
   "outputs": [],
   "source": [
    "X = final_df.drop([\"url\", \"type\", \"target\", \"subdomain\", \"domain\", \"tld\", \"fld\"], axis=1)\n",
    "y = final_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26sh-UgsHCci",
    "outputId": "54808a98-102d-4cc3-9487-71415de27033"
   },
   "outputs": [],
   "source": [
    "# Check data distribution for normality to determine how to scale the data\n",
    "# kolmogorov-smirnov test\n",
    "for column in X.columns:\n",
    "    print(f\"{column:20s}:\\t{'Normal' if kstest(X[column], 'norm')[1]<0.05 else 'Not normal':10s}:\\t{kstest(X[column], 'norm')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKcs1PoOpQzB",
    "outputId": "61dba378-f004-4d14-da42-7f0377eb167b"
   },
   "outputs": [],
   "source": [
    "# check for null variables in the training subset features\n",
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXhwZLIsuaCz"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y, random_state=42)\n",
    "\n",
    "# based on kstest we observe a normal distribution and hence use standardscaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F72QDhHNqvm_",
    "outputId": "50fa2c3c-9220-4b8c-c83c-a3e22295a202"
   },
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lziJ44vxmvD"
   },
   "source": [
    "## Step 5: Train machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-eWUzVtubz6"
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# helper function to print out evaluation metrics and confusion matrix\n",
    "def classification_report(y_test: pd.Series, y_pred: pd.Series, y_pred_proba: pd.Series):\n",
    "    evaluation_methods = [f1_score, recall_score, precision_score]\n",
    "\n",
    "    print(\"\\n****************************************************\")\n",
    "    print(f\"accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"roc_auc_score: {roc_auc_score(y_test, y_pred):.4f}\")\n",
    "    print(\"----------------------------------------------------\")\n",
    "\n",
    "    for method in evaluation_methods:\n",
    "        print(f\"{method.__name__:20s}:{method(y_test, y_pred, zero_division=0):.4f}\")\n",
    "    \n",
    "    print(\"----------------------------------------------------\")\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "\n",
    "    # plot the roc curve for the model\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "\n",
    "    # show the plot\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(\"ROC-curve\")\n",
    "    plt.show()\n",
    "    print(\"----------------------------------------------------\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    matrix = sns.heatmap(cm, annot=True, cmap=colors, fmt=\"g\", linewidths=0.5, linecolor=\"black\")\n",
    "    matrix.set_title(\"Confusion matrix\")\n",
    "    \n",
    "    matrix.set_xlabel(\"\\nPredicted values\")\n",
    "    matrix.set_ylabel(\"Actual values\")\n",
    "    \n",
    "    matrix.xaxis.set_ticklabels([\"Benign\", \"Malicious\"])\n",
    "    matrix.yaxis.set_ticklabels([\"Benign\", \"Malicious\"])\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"\\n****************************************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M47Hti3BxwrK"
   },
   "source": [
    "### 5.1 Dummy classifier\n",
    "The dummy classifier serves as the baseline model for comparison purposes. \n",
    "We use most_frequent param to always predict the most frequent label, in this case 0 for benign. In this estimator and throughout to the end, we will use a random seed of 42 to ensure that results are reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "yXdYEk3a0PYs",
    "outputId": "686c9a3b-9468-4037-effa-f45dca07824f"
   },
   "outputs": [],
   "source": [
    "dummy_classifier = DummyClassifier(strategy=\"most_frequent\", random_state=42)\n",
    "\n",
    "dummy_classifier.fit(X_train_std, y_train)\n",
    "\n",
    "dummy_test_pred = dummy_classifier.predict(X_test_std)\n",
    "dummy_test_pred_proba = dummy_classifier.predict_proba(X_test_std)\n",
    "\n",
    "classification_report(y_test, dummy_test_pred, dummy_test_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KIMBkLQyGxx"
   },
   "source": [
    "### 5.2 Random Forest\n",
    "The random forest classifier is the first algorithm that we use to classify. We use 10 fold cross-validated gridsearch to fine-tune selected parameters. The gridsearch fits a number of random forest classifiers equal to the product of all parameters combinations specified in the param_grid dictionary multiplied by the amount of folds. \n",
    "In this case 3 * 2 * 3 * 3 = 64 models * 10 folds = 640 fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "t5b9vPE_r1La",
    "outputId": "9ab1111c-d648-419a-e7b8-6751993d32f0"
   },
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 50, 100],\n",
    "    \"max_features\": [\"log2\", \"sqrt\"],\n",
    "    \"max_depth\": [2, 3, 5],\n",
    "    \"max_leaf_nodes\": [2, 4, 6]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(random_forest, param_grid=param_grid, cv=10)\n",
    "\n",
    "rf_grid.fit(X_train_std, y_train)\n",
    "\n",
    "rf_grid_pred = rf_grid.best_estimator_.predict(X_test_std)\n",
    "rf_grid_pred_proba = rf_grid.best_estimator_.predict_proba(X_test_std)\n",
    "\n",
    "classification_report(y_test, rf_grid_pred, rf_grid_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Yn4o8n0olKs",
    "outputId": "4e7fe98a-b298-42f8-c0a0-6ec8e70e5e11"
   },
   "outputs": [],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_0dxub3x42C"
   },
   "source": [
    "### 5.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GC9ZLa6mGkS3",
    "outputId": "03618748-a473-4097-b46e-aa1fd990557a"
   },
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "param_grid = {\n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"C\": [0.0001, 0.001, 0.01, 0.1],\n",
    "    \"solver\": [\"liblinear\", \"saga\"]\n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(logistic_regression, param_grid=param_grid, cv=10)\n",
    "\n",
    "lr_grid.fit(X_train_std, y_train)\n",
    "\n",
    "lr_grid_pred = lr_grid.best_estimator_.predict(X_test_std)\n",
    "lr_grid_pred_proba = lr_grid.best_estimator_.predict_proba(X_test_std)\n",
    "\n",
    "classification_report(y_test, lr_grid_pred, lr_grid_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mmQYcj_SprOG",
    "outputId": "aab51422-4535-4a76-aabb-e661faad9c2a"
   },
   "outputs": [],
   "source": [
    "lr_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZZRZyKzGHiE"
   },
   "source": [
    "### 5.4 Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "3NrIAfVJO4NN",
    "outputId": "68f0177e-9ba0-4358-e295-456c63a81026"
   },
   "outputs": [],
   "source": [
    "perceptron = MLPClassifier(random_state=42, max_iter=1000)\n",
    "\n",
    "param_grid = {\n",
    "    \"hidden_layer_sizes\": [x for x in range(5, 10)],\n",
    "    \"activation\": [\"tanh\", \"relu\"],\n",
    "    \"solver\": [\"sgd\", \"adam\"],\n",
    "    \"alpha\":  [0.0001, 0.001, 0.01],\n",
    "}\n",
    "\n",
    "perceptron_grid = GridSearchCV(perceptron, param_grid=param_grid, cv=10)\n",
    "\n",
    "perceptron_grid.fit(X_train_std, y_train)\n",
    "\n",
    "perceptron_grid_pred = perceptron_grid.best_estimator_.predict(X_test_std)\n",
    "perceptron_grid_pred_proba = perceptron_grid.best_estimator_.predict_proba(X_test_std)\n",
    "\n",
    "classification_report(y_test, perceptron_grid_pred, perceptron_grid_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3GYVdOJf51Gv",
    "outputId": "d27d8c78-c57c-4e30-8a29-72f8fad01170"
   },
   "outputs": [],
   "source": [
    "perceptron_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OKag4vMyA5A"
   },
   "source": [
    "### 5.5 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "DDdjamkRgIcd",
    "outputId": "0590d0b2-cae9-4d52-d40f-0191e846db25"
   },
   "outputs": [],
   "source": [
    "xgboost = XGBClassifier(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    \"colsample_bytree\": [0.7, 0.8],\n",
    "    \"max_depth\": [3, 4],\n",
    "    \"min_child_weight\": [4, 5],\n",
    "    \"subsample\": [i/10.0 for i in range(6, 11)],\n",
    "    \"gamma\": [i/10.0 for i in range(3, 6)],\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgboost, param_grid=param_grid, cv=10)\n",
    "\n",
    "xgb_grid.fit(X_train_std, y_train)\n",
    "\n",
    "xgb_grid_pred = xgb_grid.best_estimator_.predict(X_test_std)\n",
    "xgb_grid_pred_proba = xgb_grid.best_estimator_.predict_proba(X_test_std)\n",
    "\n",
    "classification_report(y_test, xgb_grid_pred, xgb_grid_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwwODJ1d8UBM",
    "outputId": "273e135e-8410-4e96-ed90-711d46e19529"
   },
   "outputs": [],
   "source": [
    "xgb_grid.best_params_   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "classifiers = [\n",
    "    rf_grid.best_estimator_, \n",
    "    lr_grid.best_estimator_,\n",
    "    perceptron_grid.best_estimator_, \n",
    "    xgb_grid.best_estimator_\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(10, 10))\n",
    "\n",
    "for cls, ax in zip(classifiers, axs.flatten()):\n",
    "    y_pred = cls.predict(X_test_std)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    matrix = sns.heatmap(\n",
    "        cm, \n",
    "        cmap=colors, \n",
    "        annot=True, \n",
    "        fmt=\"g\", \n",
    "        linewidths=0.5, \n",
    "        linecolor=\"black\", \n",
    "        cbar=False,\n",
    "        ax=ax,\n",
    "    )\n",
    "    matrix.set_xlabel(\"\\nPredicted values\")\n",
    "    matrix.set_ylabel(\"Actual values\")\n",
    "    \n",
    "    matrix.xaxis.set_ticklabels([\"Benign\", \"Malicious\"])\n",
    "    matrix.yaxis.set_ticklabels([\"Benign\", \"Malicious\"])\n",
    "        \n",
    "    ax.title.set_text(type(cls).__name__)\n",
    "plt.tight_layout()  \n",
    "\n",
    "plt.show()\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NNxKr8KMX3v6",
    "l7tMopblP7Jd"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
